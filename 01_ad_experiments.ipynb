{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIMnnvknkuQE"
   },
   "source": [
    "# Anomaly Detection Experiments\n",
    "Using the original trained [GEE VAE](https://github.com/munhouiani/GEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7_MXhALkgDr"
   },
   "source": [
    "## Environment Setting\n",
    "Import libraries/packages/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H85hf1lp_bIE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import psutil\n",
    "import pyspark.sql.dataframe\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import Unischema, dict_to_spark_row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import click\n",
    "from petastorm import make_reader\n",
    "from petastorm.pytorch import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_8aQKf8jGFA"
   },
   "source": [
    "## Download Data & Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "6uu9Vj4dgzzD",
    "outputId": "de881f60-b4aa-4035-a17f-5934f4de393b"
   },
   "outputs": [],
   "source": [
    "file_id=\"1ANCgjnnKaBiFwteeIdO0cF-_020jxfl2\"\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output = 'model_input.tar.gz'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfxtERf7hpGA"
   },
   "outputs": [],
   "source": [
    "!tar -xf model_input.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "gv2RIk_8wTmc",
    "outputId": "0239ec1c-974f-45f4-a8ce-3788284155c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "file_id=\"118WX53iZQiTgM-qREd7EYhIH0SJaQzfX\"\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output = 'model.tar.gz'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsDtKirrwkLD"
   },
   "outputs": [],
   "source": [
    "!tar -xf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snUK8M8iHxa-"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4hifMQyHzlS"
   },
   "outputs": [],
   "source": [
    "feature_min_max = {\n",
    "    'mean_duration': (0.0, 2042.86),\n",
    "    'mean_packet': (1.0, 109214.27272727272),\n",
    "    'mean_num_of_bytes': (28.0, 163795638.0909091),\n",
    "    'mean_packet_rate': (0.0, 17224.14377310265),\n",
    "    'mean_byte_rate': (0.0, 13902452.340182647),\n",
    "    'std_duration': (0.0, 562.7625560888366),\n",
    "    'std_packet': (0.0, 370614.95468242496),\n",
    "    'std_num_of_bytes': (0.0, 543247494.7844237),\n",
    "    'std_packet_rate': (0.0, 15783.66319664221),\n",
    "    'std_byte_rate': (0.0, 16441139.793386225),\n",
    "    'entropy_protocol': (0.0, 2.260220915066596),\n",
    "    'entropy_dst_ip': (0.0, 13.787687869067254),\n",
    "    'entropy_src_port': (0.0, 14.206227931544092),\n",
    "    'entropy_dst_port': (0.0, 14.027301292191831),\n",
    "    'entropy_flags': (0.0, 4.631615665225586)\n",
    "}\n",
    "\n",
    "\n",
    "def read_csv(spark: SparkSession, path: str) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Read csv files as spark dataframe\n",
    "\n",
    "    :param spark: spark session object\n",
    "    :param path: path of dir containing csv files\n",
    "    :type spark: SparkSession\n",
    "    :type path: str\n",
    "    :return: df\n",
    "    :rtype: pyspark.sql.dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # define csv schema\n",
    "    schema = StructType([\n",
    "        StructField('timestamp', StringType(), True),\n",
    "        StructField('duration', DoubleType(), True),\n",
    "        StructField('src_ip', StringType(), True),\n",
    "        StructField('dst_ip', StringType(), True),\n",
    "        StructField('src_port', LongType(), True),\n",
    "        StructField('dst_port', LongType(), True),\n",
    "        StructField('protocol', StringType(), True),\n",
    "        StructField('flags', StringType(), True),\n",
    "        StructField('forwarding_status', LongType(), True),\n",
    "        StructField('type_of_service', LongType(), True),\n",
    "        StructField('packet', LongType(), True),\n",
    "        StructField('num_of_bytes', LongType(), True),\n",
    "        StructField('label', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    df = (\n",
    "        spark\n",
    "            .read\n",
    "            .schema(schema)\n",
    "            .csv(path)\n",
    "    )\n",
    "\n",
    "    # convert datetime column from string to unix_timestamp\n",
    "    df = (\n",
    "        df\n",
    "            .withColumn('timestamp', unix_timestamp(col('timestamp'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def patch_time_windows(df: pyspark.sql.dataframe, window_seconds: int):\n",
    "    \"\"\"\n",
    "    Generate time window by\n",
    "    :param df: pyspark dataframe\n",
    "    :param window_seconds: window size in second\n",
    "    :type df: pyspark.sql.dataframe\n",
    "    :type window_seconds: int\n",
    "    :return: df\n",
    "    :rtype: pyspark.sql.dataframe\n",
    "    \"\"\"\n",
    "    time_window = from_unixtime(col('timestamp') - col('timestamp') % window_seconds)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "            .withColumn('time_window', time_window)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def init_local_spark():\n",
    "    # initialise local spark\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    memory_gb = psutil.virtual_memory().available // 1024 // 1024 // 1024\n",
    "    spark = (\n",
    "        SparkSession\n",
    "            .builder\n",
    "            .master('local[*]')\n",
    "            .config('spark.driver.memory', f'{memory_gb}g')\n",
    "            .config('spark.driver.host', '127.0.0.1')\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "\n",
    "def normalise(x: float, min_val: float, max_val: float) -> float:\n",
    "    norm_x = (x - min_val) / (max_val - min_val)\n",
    "    if norm_x < 0:\n",
    "        norm_x = 0.0\n",
    "    elif norm_x > 1.0:\n",
    "        norm_x = 1.0\n",
    "\n",
    "    return norm_x\n",
    "\n",
    "\n",
    "def row_generator(x):\n",
    "    time_window, src_ip, feature, label = x\n",
    "    return {\n",
    "        'time_window': time_window,\n",
    "        'src_ip': src_ip,\n",
    "        'feature': np.expand_dims(np.array(feature, dtype=np.float32), axis=0),\n",
    "        'label': label,\n",
    "    }\n",
    "\n",
    "\n",
    "def change_df_schema(spark: SparkSession, schema: Unischema, df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    rows_rdd = (\n",
    "        df\n",
    "            .rdd\n",
    "            .map(row_generator)\n",
    "            .map(lambda x: dict_to_spark_row(schema, x))\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame(\n",
    "        rows_rdd,\n",
    "        schema.as_spark_schema()\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_parquet_for_petastorm_parquet(spark: SparkSession, df: pyspark.sql.DataFrame, output_path: str,\n",
    "                                       schema: Unischema):\n",
    "    #output_path = Path(output_path).absolute().as_uri()\n",
    "    output_path = 'file://' + str(Path(output_path).absolute())\n",
    "    with materialize_dataset(spark, output_path, schema, row_group_size_mb=256):\n",
    "        (\n",
    "            df\n",
    "                .write\n",
    "                .mode('overwrite')\n",
    "                .parquet(output_path)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fipHC1E4Bt54"
   },
   "source": [
    "## Define Model\n",
    "GEE VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPdhQMe_B2og"
   },
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return BCE + KLD\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['feature']\n",
    "        recon_x, mu, logvar = self(x)\n",
    "        loss = self.loss_function(recon_x, x, mu, logvar)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "\n",
    "class Encoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            # layer 1\n",
    "            nn.Linear(\n",
    "                in_features=69,\n",
    "                out_features=512\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            # layer 2\n",
    "            nn.Linear(\n",
    "                in_features=512,\n",
    "                out_features=512\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            # layer 3\n",
    "            nn.Linear(\n",
    "                in_features=512,\n",
    "                out_features=1024\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # output\n",
    "        self.mu = nn.Linear(\n",
    "            in_features=1024,\n",
    "            out_features=100\n",
    "        )\n",
    "        self.logvar = nn.Linear(\n",
    "            in_features=1024,\n",
    "            out_features=100\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc(x)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "\n",
    "class Decoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            # layer 1\n",
    "            nn.Linear(\n",
    "                in_features=100,\n",
    "                out_features=1024\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            # layer 2\n",
    "            nn.Linear(\n",
    "                in_features=1024,\n",
    "                out_features=512\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            # layer 3\n",
    "            nn.Linear(\n",
    "                in_features=512,\n",
    "                out_features=512\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            # output\n",
    "            nn.Linear(\n",
    "                in_features=512,\n",
    "                out_features=69\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKqC2_SozWan"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qE0JZdV3zWao"
   },
   "outputs": [],
   "source": [
    "anomaly_detector_model_path = 'model/vae.model'\n",
    "data_path = 'model_input/test.model_input.parquet'\n",
    "\n",
    "# get number of cores\n",
    "num_cores = psutil.cpu_count(logical=True)\n",
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmrA2uJRzWaq"
   },
   "source": [
    "## Load Trained GEE VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8JrBSTXzWaq",
    "outputId": "9852151d-a062-4fe7-8f94-4b43d84f3b55"
   },
   "outputs": [],
   "source": [
    "anomaly_detector_model = VAE.load_from_checkpoint(checkpoint_path=anomaly_detector_model_path, map_location=torch.device(gpu))\n",
    "anomaly_detector_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyGQV0SPzWav"
   },
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0CQeqtzzWaw"
   },
   "outputs": [],
   "source": [
    "def calc_recon_loss(recon_x, x, logvar = None, mu = None, loss_type: str = 'mse') -> list:\n",
    "    \"\"\"\n",
    "    Return the reconstruction loss\n",
    "\n",
    "    :param recon_x: reconstructed x, output from model\n",
    "    :param x: original x\n",
    "    :param logvar: variance, output from model, ignored when loss_type isn't 'bce+kd'\n",
    "    :param mu: mean, output from model, ignored when loss_type isn't 'bce+kd'\n",
    "    :param loss_type: method to compute loss, option: 'bce', 'mse', 'bce+kd'\n",
    "    :return: list of reconstruct errors\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    loss_type = loss_type.lower()\n",
    "\n",
    "    # 69 is the number of features\n",
    "    if loss_type == 'mse':\n",
    "        recon_error = F.mse_loss(recon_x, x, reduction='none').view(-1, 69).mean(dim=1)\n",
    "    elif loss_type == 'bce':\n",
    "        recon_error = F.binary_cross_entropy(recon_x, x, reduction='none').view(-1, 69).mean(dim=1)\n",
    "    elif loss_type == 'bce+kd':\n",
    "        bce = F.binary_cross_entropy(recon_x, x, reduction='none').view(-1, 69).mean(dim=1)\n",
    "        kd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        recon_error = bce + kd\n",
    "    else:\n",
    "        raise Exception('Invalid loss type: only support \"mse\", \"bce\", or \"bce+kd\"')\n",
    "\n",
    "    return recon_error.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "fRnELAIYJtf5",
    "outputId": "a011182c-6f42-4f26-b465-39977b8dc926"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reader = make_reader(\n",
    "    'file://' + str(Path(data_path).absolute()), reader_pool_type='process', workers_count=num_cores,\n",
    "    pyarrow_serialize=True, num_epochs=1\n",
    ")\n",
    "dataloader = DataLoader(reader, batch_size=300, shuffling_queue_capacity=4096)\n",
    "\n",
    "labels = []\n",
    "xes = []\n",
    "mse = []\n",
    "\n",
    "for data in dataloader:\n",
    "    data_x = data['feature']\n",
    "    data_labels = data['label']\n",
    "    xes.extend(np.squeeze(data_x.numpy()))\n",
    "    labels.extend(data_labels)\n",
    "    reconstruction, mu, logvar = anomaly_detector_model(data_x)\n",
    "    mse.extend(calc_recon_loss(reconstruction, data_x, loss_type='mse'))\n",
    "\n",
    "features = []\n",
    "for item in xes:\n",
    "    features.append(np.squeeze(item).astype(float))\n",
    "test_data = pd.DataFrame(\n",
    "    {\n",
    "        'labels': labels,\n",
    "        'features': features,\n",
    "        'mse': mse\n",
    "    }\n",
    ")\n",
    "test_data.to_feather('results_ad_test.feather.with_mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHhaBnIyy-mq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
